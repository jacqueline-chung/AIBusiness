{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADDITIONAL -DON'T RUN - ITM 703 Course Project Code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1HO2agSG3V1g",
        "1PJg-SOA9y0N",
        "UkXl2vzgnF2s",
        "cHU1xRtZ-Fi2",
        "CxuuMV0ZKmR7",
        "yDSF4L6dGlf1",
        "8s39tcEQjG6l",
        "Wk3Q7RT0i0pP",
        "akbvJfggGyWJ",
        "KzPT3UO8-Jhd",
        "L4roflcD-OER",
        "qwv8Y4-N-_ZC",
        "bxvO_NLYii3S",
        "VXq07wcJ-TiK",
        "EELtYNGLkYbT",
        "5rPnjDjxkLhM"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacqueline-chung/AIBusiness/blob/master/ADDITIONAL_DON'T_RUN_ITM_703_Course_Project_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HO2agSG3V1g",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries and Load TensorBoard**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NXee3VMTMxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "35d685cd-aa84-4bdc-9f97-0623ab6483a7"
      },
      "source": [
        "# Import libraries for building neural networks\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "\n",
        "# Import for TensorBoard\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "from tensorboard import summary as summary_lib\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "# Import for ROC and AUC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Mount your google drive to your Google Colab\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Import for metrics\n",
        "from keras import backend as K\n",
        "\n",
        "# Import for Data Exploration\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Get the id of your file by right click on it on your google drive \n",
        "# Link: https://drive.google.com/open?id=1KuReb5jz_mu0qbzQsJMjjHLMzU2E67lC\n",
        "downloaded2 = drive.CreateFile({'id':'1KuReb5jz_mu0qbzQsJMjjHLMzU2E67lC'}) \n",
        "downloaded2.GetContentFile('heart.csv')  \n",
        "\n",
        "downloaded = drive.CreateFile({'id':'1KuReb5jz_mu0qbzQsJMjjHLMzU2E67lC'}) \n",
        "downloaded.GetContentFile('cd.csv')  \n",
        "\n",
        "# Use Tensor 2.0\n",
        "debug = True\n",
        "_verbose=1 if debug else 0\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "# Load inline\n",
        "%matplotlib inline\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PJg-SOA9y0N",
        "colab_type": "text"
      },
      "source": [
        "# **Split Training and Test Data**\n",
        "\n",
        "\n",
        "*  Split the data randomly, where training data is 70% and test data is 30%\n",
        "* Print the data that was split between training and test\n",
        "\n",
        "Please refer to the followin link for more infomation about using loc function,\n",
        "https://www.w3resource.com/pandas/series/series-loc.php\n",
        "on this link you can find a section to use a bolean values to select the rows\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31hPCxp6SSB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following code generates a list with same length as our data, and randomly assigns true or false to them \n",
        "split = 0.7\n",
        "# Compare with split because if loading msk, you will get T or F -> subset dataframes\n",
        "msk = np.random.rand(len(data)) < split\n",
        "\n",
        "# Shuffle the entire data set (applies to both train & test)\n",
        "#  Sample method return a random sample of data , and by setting fraction to 1, we'll get the whole data in a shuffled way\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Using panadas loc function to subset our list\n",
        "# Training Data\n",
        "train_labels = data.loc[msk, data.columns =='target']\n",
        "train_data  = data.loc[msk, data.columns !='target']\n",
        "# Testing Data\n",
        "test_labels = data.loc[~msk, data.columns =='target']\n",
        "test_data  = data.loc[~msk, data.columns !='target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHSfynmaSUCz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "b22821a8-7195-4db8-826b-e5eac0a77aba"
      },
      "source": [
        "# Look at first rows of data to check training and test data\n",
        "print('Test Data:', test_data.head(1))\n",
        "print('Test Target Class:', test_labels.head(1))\n",
        "print('Training Data:', train_data.head(1))\n",
        "print('Training Target Class:', train_labels.head(1))\n",
        "\n",
        " # Random numbers that are generated (based on # of rows in dataset)\n",
        "print(msk[0:5])\n",
        "# Check shape of each dataset\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "# Check the number of values in total\n",
        "print(test_data.shape[0]+train_data.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Data:    age  sex  cp  trestbps  chol  fbs  ...  thalach  exang  oldpeak  slope  ca  thal\n",
            "1   52    1   1       134   201    0  ...      158      0      0.8      2   1     2\n",
            "\n",
            "[1 rows x 13 columns]\n",
            "Test Target Class:    target\n",
            "1       1\n",
            "Training Data:    age  sex  cp  trestbps  chol  fbs  ...  thalach  exang  oldpeak  slope  ca  thal\n",
            "0   48    1   1       110   229    0  ...      168      0      1.0      0   0     3\n",
            "\n",
            "[1 rows x 13 columns]\n",
            "Training Target Class:    target\n",
            "0       0\n",
            "[ True False False  True  True]\n",
            "(197, 13)\n",
            "(106, 13)\n",
            "303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkXl2vzgnF2s",
        "colab_type": "text"
      },
      "source": [
        "### Cross Entropy\n",
        "\n",
        "(Code doesn't work)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5PMiWLdm61V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries for Cross-Entropy\n",
        "from sklearn.model_selection import LeaveOneOut,KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Declare features for data\n",
        "feature_names = data.columns\n",
        "X = data.drop(data.columns[-1], axis=1)\n",
        "y = data['target']\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
        "# X is the feature set and y is the target\n",
        "for train_index, test_index in skf.split(X,y): \n",
        "    print(\"Train:\", train_index, \"Validation:\", test_index) \n",
        "    train_data, test_data = X[train_index], X[test_index] \n",
        "    train_labels, test_labels = y[train_index], y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHU1xRtZ-Fi2",
        "colab_type": "text"
      },
      "source": [
        "# **Normalization**\n",
        "The numerical values stay between [RANGE]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNqZi7coSXzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalizing our data: setting the axis to 0 means that the mean and std \n",
        "#  should calculated towards rows not columns\n",
        "mean = train_data.mean(axis=0)\n",
        "std = train_data.std(axis=0)\n",
        "train_data = (train_data - mean) / std\n",
        "test_data = (test_data - mean) / std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wkbc4A5SYRh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb334c62-a1cc-49dc-960b-4a8325d7b1f0"
      },
      "source": [
        "# Check training and test data after normalization\n",
        "print(train_data.head(1))\n",
        "print(test_data.head(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        age       sex        cp  ...     slope        ca      thal\n",
            "0 -0.681558  0.740529  0.069829  ... -2.287915 -0.679177  1.175925\n",
            "\n",
            "[1 rows x 13 columns]\n",
            "      age       sex        cp  trestbps  ...   oldpeak     slope        ca      thal\n",
            "1 -0.2377  0.740529  0.069829  0.154293  ... -0.184308  0.920051  0.342181 -0.455464\n",
            "\n",
            "[1 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CxuuMV0ZKmR7"
      },
      "source": [
        "# **Define Hyperparameters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shbRzECMn9Ld",
        "colab_type": "text"
      },
      "source": [
        "### 1 Hidden Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WJE1mAtOHWy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df33ece-7efa-444e-ac68-2d880e58eb14"
      },
      "source": [
        "'''\n",
        "# We create the parameters that we would like to try different values for.\n",
        "# we use the HParam method to parametrize our model so we investigate the effect of different parameters. \n",
        "\n",
        "# Adjust # of neurons in input (first) layer\n",
        "HP_INPUT = hp.HParam('input', hp.Discrete([4,16,24]))\n",
        "\n",
        "# Adjust # of neurons in the hidden (second) layer \n",
        "HP_HIDDEN1 = hp.HParam('hidden 1', hp.Discrete([4,8])) \n",
        "\n",
        "# Use different optimizer to see which one gives better accuracy\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
        "\n",
        "# Use the most common activation functions\n",
        "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu','sigmoid']))\n",
        "\n",
        "# Use 5 epochs for now\n",
        "HP_EPOCH = hp.HParam('epoch', hp.Discrete([5]))\n",
        "\n",
        "# Use common loss function\n",
        "HP_LOSS = hp.HParam('loss', hp.Discrete(['sparse_categorical_crossentropy']))\n",
        "\n",
        "# Our goal is to find the values for the above paramters that gives the best accuracy on the test data\n",
        "ACCURACY_METRIC = 'accuracy'\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# We create the parameters that we would like to try different values for.\\n# we use the HParam method to parametrize our model so we investigate the effect of different parameters. \\n\\n# Adjust # of neurons in input (first) layer\\nHP_INPUT = hp.HParam('input', hp.Discrete([4,16,24]))\\n\\n# Adjust # of neurons in the hidden (second) layer \\nHP_HIDDEN1 = hp.HParam('hidden 1', hp.Discrete([4,8])) \\n\\n# Use different optimizer to see which one gives better accuracy\\nHP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\\n\\n# Use the most common activation functions\\nHP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu','sigmoid']))\\n\\n# Use 5 epochs for now\\nHP_EPOCH = hp.HParam('epoch', hp.Discrete([5]))\\n\\n# Use common loss function\\nHP_LOSS = hp.HParam('loss', hp.Discrete(['sparse_categorical_crossentropy']))\\n\\n# Our goal is to find the values for the above paramters that gives the best accuracy on the test data\\nACCURACY_METRIC = 'accuracy'\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s39tcEQjG6l",
        "colab_type": "text"
      },
      "source": [
        "### 2 Hidden Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "imlq-Q0PGlf8",
        "colab": {}
      },
      "source": [
        "# This shows us the best hyperparameters for the neural network classifiers\n",
        "\n",
        "# We create the parameters that we would like to try different values for.\n",
        "# We use the HParam method to parametrize our model so we investigate the effect of different parameters. \n",
        "\n",
        "# Adjust # of neurons in input (first) layer\n",
        "HP_INPUT = hp.HParam('input', hp.Discrete([500,750,1000]))\n",
        "\n",
        "# Adjust # of neurons in the first hidden (second) layer \n",
        "HP_HIDDEN1  = hp.HParam('hidden 1', hp.Discrete([8,16,50]))\n",
        "\n",
        "# Adjust # of neurons in the second hidden (third) layer \n",
        "HP_HIDDEN2 = hp.HParam('hidden 2', hp.Discrete([8,16,50]))\n",
        "\n",
        "# Use different optimizer to see which one gives better accuracy\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam','sgd']))\n",
        "\n",
        "# Use Activation function called Tanh and ReLU\n",
        "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['tanh', 'relu']))\n",
        "\n",
        "# Use 5 and 10 epochs\n",
        "HP_EPOCH = hp.HParam('epoch', hp.Discrete([5,10]))\n",
        "\n",
        "# Use common loss function: Binary Cross-Entropy, Hinge, Squared Hinge\n",
        "HP_LOSS = hp.HParam('loss', hp.Discrete(['binary_crossentropy','hinge','squared_hinge']))\n",
        "\n",
        "# Declare 'accuracy' variable\n",
        "ACC_METRIC = 'accuracy'\n",
        "\n",
        "# Our goal is to find the values for the above paramters that gives the best performance on the test data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk3Q7RT0i0pP",
        "colab_type": "text"
      },
      "source": [
        "### 3 Hidden Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgOfg8DEi3W2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This shows us the best hyperparameters for the neural network classifiers\n",
        "\n",
        "# We create the parameters that we would like to try different values for.\n",
        "# We use the HParam method to parametrize our model so we investigate the effect of different parameters. \n",
        "\n",
        "# Adjust # of neurons in input (first) layer\n",
        "HP_INPUT = hp.HParam('input', hp.Discrete([500,750,1000]))\n",
        "\n",
        "# Adjust # of neurons in the first hidden (second) layer \n",
        "HP_HIDDEN1  = hp.HParam('hidden 1', hp.Discrete([8,16,50]))\n",
        "\n",
        "# Adjust # of neurons in the second hidden (third) layer \n",
        "HP_HIDDEN2 = hp.HParam('hidden 2', hp.Discrete([8,16,50]))\n",
        "\n",
        "# Adjust # of neurons in the third hidden (fourth) layer \n",
        "HP_HIDDEN2 = hp.HParam('hidden 3', hp.Discrete([8,16,50]))\n",
        "\n",
        "# Use different optimizer to see which one gives better accuracy\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam','sgd']))\n",
        "\n",
        "# Use Activation function called Tanh and ReLU\n",
        "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['tanh', 'relu']))\n",
        "\n",
        "# Use 5 and 10 epochs\n",
        "HP_EPOCH = hp.HParam('epoch', hp.Discrete([5,10]))\n",
        "\n",
        "# Use common loss function: Binary Cross-Entropy, Hinge, Squared Hinge\n",
        "HP_LOSS = hp.HParam('loss', hp.Discrete(['binary_crossentropy','hinge','squared_hinge']))\n",
        "\n",
        "# Declare 'accuracy' variable\n",
        "ACC_METRIC = 'accuracy'\n",
        "\n",
        "# Our goal is to find the values for the above paramters that gives the best performance on the test data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "akbvJfggGyWJ"
      },
      "source": [
        "# **Define Metrics**\n",
        "Create functions for Recall, Precision, F1 Score to measure performance for models.\n",
        "\n",
        "The 3 following metrics, in addition to classification accuracy, that are commonly required for a neural network model on a binary classification problem are:\n",
        "\n",
        "* Precision\n",
        "* Recall\n",
        "* F1 Score\n",
        "\n",
        "We will calculate these three metrics, as well as classification accuracy using the scikit-learn metrics API, as well as loss.\n",
        "\n",
        "Links:\n",
        "* https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5NlAbQxGvrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define recall_m using training and test labels \n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "# Define precision_m using training and test labels \n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "# Define f1_m using training and test labels \n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzPT3UO8-Jhd",
        "colab_type": "text"
      },
      "source": [
        "# **Parameter Sensitivity Analysis**\n",
        "Change Hyper Parameters for TensorBoard\n",
        "This will overide the initial hyperparameters\n",
        "Due to the limitations of Google Colab, we broke up the code in different files to run each iteration, so these hyperparameters were actually done separately, but it was what we tested to get our final results.\n",
        "\n",
        "* no of epochs\n",
        "* no of neurons (first)\n",
        "* no of neurons (hidden)\n",
        "* activation function\n",
        "* learning rate (if you have time)\n",
        "* no of layers\n",
        "\n",
        "Try adding slowly, so the program doesn't crash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiBoKrJnNFYE",
        "colab_type": "text"
      },
      "source": [
        "No of neurons (first layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs-7aV7lCaar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adjust # of neurons in input (first) layer \n",
        "HP_INPUT = hp.HParam('input', hp.Discrete([3,4,8,9,12,16,18,25,30,50,75,100,150,250,500,750,1000]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh9TO9DNNIc-",
        "colab_type": "text"
      },
      "source": [
        "No of Neurons (Hidden Layer)\n",
        "* This is advance: The number can be decided using formulas (i.e. 75% of input should be used for units for hidden (but not fixed since it's based on probaility))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwWdl63NCLKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adjust # of neurons in hidden (second) layer (smaller increments)\n",
        "HP_HIDDEN1 = hp.HParam('hidden 1', hp.Discrete([3,4,8,9,12,16,18,25,30,50,75,100,150,250,500,750,1000])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gSNVLytMWA5",
        "colab_type": "text"
      },
      "source": [
        "No of Epochs\n",
        "\n",
        "Depending on the optimizer and activation functions, the # of epochs may improve or increase overfitting. However, it seems that having more epochs is good to an extent. Having too little can make it more difficult for the model to learn. We have to find the sweet spot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XfBhfJQijH8V",
        "colab": {}
      },
      "source": [
        "HP_EPOCH = hp.HParam('epoch', hp.Discrete([5, 10, 30, 50, 75, 100, 200]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J20RA72NL5b",
        "colab_type": "text"
      },
      "source": [
        "Activation Functions \n",
        "\n",
        "We're starting with the most common ones, then we will work our way to other functions:\n",
        "*  Sigmoid \n",
        "*  ReLU\n",
        "* Softmax (use less - not as common)\n",
        "* Tanh\n",
        "\n",
        "Links:\n",
        "* https://keras.io/activations/\n",
        "* https://towardsdatascience.com/hyper-parameters-in-action-a524bf5bf1c\n",
        "* https://www.tensorflow.org/api_docs/python/tf/keras/activations?version=nightly\n",
        "\n",
        "Note: Not all the activation functions will work due to the shape of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15yXp7vNUER0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu', 'sigmoid', 'softmax', 'tanh']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3PuHNv2fjxk",
        "colab_type": "text"
      },
      "source": [
        "Multiple Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKuKxe8Qfik8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change the activation functions for each layer\n",
        "HP_ACTIVATION1 = hp.HParam('activation (input)', hp.Discrete(['relu', 'tanh','sigmoid']))\n",
        "\n",
        "HP_ACTIVATION2 = hp.HParam('activation (hidden 1)', hp.Discrete(['relu', 'tanh','sigmoid']))\n",
        "\n",
        "HP_ACTIVATION3 = hp.HParam('activation (hidden 2)', hp.Discrete(['relu', 'tanh','sigmoid']))\n",
        "\n",
        "HP_ACTIVATION4 = hp.HParam('activation (hidden 3)', hp.Discrete(['relu', 'tanh','sigmoid']))\n",
        "\n",
        "HP_ACTIVATION4 = hp.HParam('activation (hidden 3)', hp.Discrete(['relu', 'tanh','sigmoid']))\n",
        "\n",
        "HP_ACTIVATION5 = hp.HParam('activation (output)', hp.Discrete(['relu', 'tanh','sigmoid']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcyAPSLSgRAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following changes would be made to the structure:\n",
        "\n",
        "# Build model with hyperparameters and activation functions\n",
        "def build_model(hparams):\n",
        "  model = keras.Sequential([\n",
        "    keras.layers.Dense(hparams[HP_INPUT], hparams[HP_ACTIVATION1],  # put our parameter (replace parameter with # of parameter)\n",
        "                       input_shape=(train_data.shape[1],)), # Input Layer\n",
        "    keras.layers.Dense(hparams[HP_HIDDEN1], hparams[HP_ACTIVATION2]), # Hidden Layer\n",
        "    #keras.layers.Dense(hparams[HP_HIDDEN2], hparams[HP_ACTIVATION]), # Hidden Layer\n",
        "    #keras.layers.Dense(hparams[HP_HIDDEN3], hparams[HP_ACTIVATION]), # Hidden Layer\n",
        "    keras.layers.Dense(2, hparams[HP_ACTIVATION3]) # Output Layer\n",
        "])\n",
        "  \n",
        "  # Compiling model; Calculate Accuracy; Optimizer optimizes loss function \n",
        "  model.compile(hparams[HP_OPTIMIZER],hparams[HP_LOSS],ACCURACY_METRIC)\n",
        "  #model.compile([HP_OPTIMIZER](hparams[HP_LR]),hparams[HP_LOSS],ACCURACY_METRIC)\n",
        "\n",
        "# Fit and train model and save metrics in history\n",
        "  model.fit(train_data, train_labels, hparams[HP_EPOCH], \n",
        "            validation_split = 0.2, verbose = 1)\n",
        "  \n",
        "  _, accuracy = model.evaluate(test_data, test_labels)\n",
        "  return accuracy # Output model\n",
        "\n",
        "# With statement\n",
        "with tf.summary.create_file_writer('./hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "      hparams=[HP_INPUT,HP_HIDDEN1,HP_OPTIMIZER, HP_ACTIVATION1, HP_ACTIVATION2, HP_ACTIVATION3,HP_ACTIVATION4, HP_ACTIVATION5,  HP_EPOCH, HP_LOSS],\n",
        "      metrics=[hp.Metric(ACCURACY_METRIC, display_name='Accuracy')]\n",
        "  )\n",
        "\n",
        "# This function will execute the model builing method with different set of paratemer and log the results to the folder.\n",
        "# With statement helps you to keep your log file open while executing the commands in the with block\n",
        "def run(run_dir, hparams):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # Record the values used in this trial\n",
        "    loss, recall, precision, f1, accuracy = build_model(hparams)\n",
        "    tf.summary.scalar('loss', loss, step=1)\n",
        "    tf.summary.scalar('recall', recall, step=1) # Scalar: a data class; different value\n",
        "    tf.summary.scalar('precision', precision, step=1) # Scalar: a data class; different value\n",
        "    tf.summary.scalar('f1', f1, step=1)\n",
        "    tf.summary.scalar(ACC_METRIC, accuracy, step=1)\n",
        "\n",
        "# Run our model by using hyperparameters that defined earlier\n",
        "# Combined all of them in for loops to run the different combinations of parameters\n",
        "session_num = 0\n",
        "for input1 in HP_INPUT.domain.values:\n",
        "  for hidden1 in HP_HIDDEN1.domain.values:\n",
        "    for optimizer in HP_OPTIMIZER.domain.values:\n",
        "        for activation1 in HP_ACTIVATION1.domain.values:\n",
        "                for activation2 in HP_ACTIVATION2.domain.values:\n",
        "                    for activation3 in HP_ACTIVATION3.domain.values:\n",
        "                        for activation4 in HP_ACTIVATION4.domain.values:\n",
        "                            for activation5 in HP_ACTIVATION5.domain.values:\n",
        "                                for epoch in HP_EPOCH.domain.values:\n",
        "                                    for loss in HP_LOSS.domain.values:\n",
        "                                            hparams = {\n",
        "                                                HP_INPUT: input1,\n",
        "                                                HP_HIDDEN1: hidden1,\n",
        "                                                HP_OPTIMIZER: optimizer,\n",
        "                                                HP_ACTIVATION1: activation1,\n",
        "                                                HP_ACTIVATION2: activation2,\n",
        "                                                HP_ACTIVATION3: activation3, \n",
        "                                                HP_ACTIVATION4: activation4, \n",
        "                                                HP_ACTIVATION5: activation5,                                    \n",
        "                                    }\n",
        "                                    run_name = \"run-%d\" % session_num \n",
        "                                    print('--- Starting trial: %s' % run_name) \n",
        "                                    print({h.name: hparams[h] for h in hparams}) # this runs the parameters\n",
        "                                    run('./hparam_tuning/' + run_name, hparams)\n",
        "                                    session_num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPKIS9oJNYHe",
        "colab_type": "text"
      },
      "source": [
        "Learning rate - more advanced\n",
        "\n",
        "Higher learnerning rate will decrease performance and increase overfitting, while lower learning rate will have less reliance on previous instances. This will help decrease overfitting. However, it is possible to underfit, it would make the model too simple and it would need more epochs (and vice versa).\n",
        "This is why we need the if statements to run, instead of the hyperparameters. \n",
        "\n",
        "Links:\n",
        "* https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/\n",
        "* https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/scalars_and_keras.ipynb\n",
        "\n",
        "(The code didn't work)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Alt_svU2h6zc",
        "colab": {}
      },
      "source": [
        "HP_LR = hp.HParam('learning rate', hp.Discrete([0.1,0.5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfeKdNKhj9ZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "  \"\"\"\n",
        "  Returns a custom learning rate that decreases as epochs progress.\n",
        "\n",
        "  Should I use hyperparams??\n",
        "  \"\"\"\n",
        "  learning_rate = 0.2\n",
        "  if epoch > 10:\n",
        "    learning_rate = 0.02\n",
        "  if epoch > 20:\n",
        "    learning_rate = 0.01\n",
        "  if epoch > 50:\n",
        "    learning_rate = 0.005\n",
        "\n",
        "  tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
        "  return learning_rate\n",
        "\n",
        "lr_callback = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "tensorboard_callback = keras.callbacks.TensorBoard()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jn2OVSIjnVm",
        "colab_type": "text"
      },
      "source": [
        "Loss - if you have time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joPgwUm-jpTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HP_LOSS = hp.HParam('loss', hp.Discrete(['sparse_categorical_crossentropy', 'binary_crossentropy','hinge','squared_hinge','categorical_crossentropy','kullback_leibler_divergence']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD_0t4foFiW4",
        "colab_type": "text"
      },
      "source": [
        "No of Hidden Layers\n",
        "\n",
        "\n",
        "*   1\n",
        "*   2\n",
        "* 3\n",
        "\n",
        "From research, it says, on average, 2 hidden layers are best. It depends how complicated the data is, but since this data is pretty small, we will stick with 2 to 3. Having more hidden layers may lead to overfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEy7ws87pMbp",
        "colab_type": "text"
      },
      "source": [
        "No of Neurons (Hidden Layer 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ojidXGdipdrs",
        "colab": {}
      },
      "source": [
        "# Adjust # of neurons in hidden (second) layer (smaller increments)\n",
        "HP_HIDDEN2 = hp.HParam('hidden 2', hp.Discrete([3,4,8,9,12,16,18,25,30,50,75,100,150,250,500,750,1000])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HL8EGK_mpXip"
      },
      "source": [
        "No of Neurons (Hidden Layer 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rJFbkid4pfJF",
        "colab": {}
      },
      "source": [
        "# Adjust # of neurons in hidden (second) layer (smaller increments)\n",
        "HP_HIDDEN3 = hp.HParam('hidden 3', hp.Discrete([3,4,8,9,12,16,18,25,30,50,75,100,150,250,500,750,1000])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4roflcD-OER",
        "colab_type": "text"
      },
      "source": [
        "# **Neural Network Classifier**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwv8Y4-N-_ZC",
        "colab_type": "text"
      },
      "source": [
        "### 2 Hidden Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqyEguk6-xDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build model with hyperparameters and activation functions\n",
        "def build_model(hparams):\n",
        "  model = keras.Sequential([\n",
        "    keras.layers.Dense(hparams[HP_INPUT], hparams[HP_ACTIVATION],  # put our parameter (replace parameter with # of parameter)\n",
        "                       input_shape=(train_data.shape[1],)), # Input Layer\n",
        "    keras.layers.Dense(hparams[HP_HIDDEN1], hparams[HP_ACTIVATION]), # Hidden Layer\n",
        "    keras.layers.Dense(hparams[HP_HIDDEN2], hparams[HP_ACTIVATION]), # Hidden Layer\n",
        "    #keras.layers.Dense(hparams[HP_HIDDEN3], hparams[HP_ACTIVATION]), # Hidden Layer\n",
        "    keras.layers.Dense(2, hparams[HP_ACTIVATION]) # Output Layer\n",
        "])\n",
        "  \n",
        "  # Compiling model by calculating accuracy, recall, precision, and F1 score\n",
        "  # Use Optimizer optimizes loss function \n",
        "  model.compile(hparams[HP_OPTIMIZER],hparams[HP_LOSS],metrics=[recall_m, precision_m, f1_m, ACC_METRIC])\n",
        "\n",
        "# Fit and train model and save metrics in history\n",
        "  model.fit(train_data, train_labels, hparams[HP_EPOCH], \n",
        "            validation_split = 0.2, verbose = 1)\n",
        "  \n",
        "  # Evaluate the model using test data\n",
        "  loss = model.evaluate(test_data, test_labels)\n",
        "  _,recall, precision, f1, accuracy = model.evaluate(test_data, test_labels)\n",
        "\n",
        "  # Get loss, recall, precision, F1 score, and accuracy after evaluating model against test data\n",
        "  return loss[0], recall, precision, f1, accuracy # Output model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bxvO_NLYii3S"
      },
      "source": [
        "### 3 Hidden Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S20uCraWihJh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "193e54c4-f36d-42c5-d732-7643712591a9"
      },
      "source": [
        "## Build model with hyperparameters and activation functions\n",
        "def build_model(hparams):\n",
        "  model = keras.Sequential([\n",
        "    keras.layers.Dense(hparams[HP_INPUT], hparams[HP_ACTIVATION],  # put our parameter (replace parameter with # of parameter)\n",
        "                       input_shape=(train_data.shape[1],)), # Input Layer\n",
        "    keras.layers.Dense(hparams[HP_HIDDEN1], hparams[HP_ACTIVATION]), # Hidden Layer\n",
        "    keras.layers.Dense(hparams[HP_HIDDEN2], hparams[HP_ACTIVATION]), # Hidden Layer\n",
        "    keras.layers.Dense(hparams[HP_HIDDEN3], hparams[HP_ACTIVATION]), # Hidden Layer\n",
        "    keras.layers.Dense(2, hparams[HP_ACTIVATION]) # Output Layer\n",
        "])\n",
        "  \n",
        "  # Compiling model; Calculate Accuracy; Optimizer optimizes loss function \n",
        "  model.compile(hparams[HP_OPTIMIZER],hparams[HP_LOSS],metrics=[recall_m, precision_m, f1_m, ACC_METRIC])\n",
        "  #model.compile([HP_OPTIMIZER](hparams[HP_LR]),hparams[HP_LOSS],ACCURACY_METRIC)\n",
        "\n",
        "# Fit and train model and save metrics in history\n",
        "  model.fit(train_data, train_labels, hparams[HP_EPOCH], \n",
        "            validation_split = 0.2, verbose = 1)\n",
        "  \n",
        "  loss = model.evaluate(test_data, test_labels)\n",
        "  _,recall, precision, f1, accuracy = model.evaluate(test_data, test_labels)\n",
        "\n",
        "  return loss[0], recall, precision, f1, accuracy # Output model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXq07wcJ-TiK",
        "colab_type": "text"
      },
      "source": [
        "# **Add Metrics**\n",
        "* Accuracy \n",
        "* Running Time (Epoch)\n",
        "* Precision\n",
        "* Recall\n",
        "* F1 Score\n",
        "* Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUVONQQgSb7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# With statement by adding the hyperparameters in the summary\n",
        "with tf.summary.create_file_writer('./hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_INPUT,HP_HIDDEN1, HP_HIDDEN2,  HP_OPTIMIZER, HP_ACTIVATION, HP_EPOCH, HP_LOSS],\n",
        "    metrics=[hp.Metric('loss', display_name='loss'),\n",
        "             hp.Metric('recall', display_name='recall'),\n",
        "             hp.Metric('precision', display_name='precision'),\n",
        "             hp.Metric('f1', display_name='f1_score'),\n",
        "             hp.Metric(ACC_METRIC, display_name='accuracy')]\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kxkcurkCHsOa"
      },
      "source": [
        "## **Run Models with Hyparam**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EELtYNGLkYbT",
        "colab_type": "text"
      },
      "source": [
        "## 2 Hidden Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehPyqlw2Seit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function will execute the model builing method with different set of paratemer and log the results to the folder.\n",
        "# With statement helps you to keep your log file open while executing the commands in the with block\n",
        "def run(run_dir, hparams):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # Record the values used in this trial\n",
        "    loss, recall, precision, f1, accuracy = build_model(hparams)\n",
        "    tf.summary.scalar('loss', loss, step=1)\n",
        "    tf.summary.scalar('recall', recall, step=1) # Scalar: a data class; different value\n",
        "    tf.summary.scalar('precision', precision, step=1) # Scalar: a data class; different value\n",
        "    tf.summary.scalar('f1', f1, step=1)\n",
        "    tf.summary.scalar(ACC_METRIC, accuracy, step=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIBQpGYiSgQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run our model by using hyperparameters that defined earlier\n",
        "# Combined all of them in for loops to run the different combinations of parameters\n",
        "session_num = 0\n",
        "\n",
        "for input1 in HP_INPUT.domain.values:\n",
        "  for hidden1 in HP_HIDDEN1.domain.values:\n",
        "      for hidden2 in HP_HIDDEN2.domain.values:\n",
        "            for optimizer in HP_OPTIMIZER.domain.values:\n",
        "                for activation in HP_ACTIVATION.domain.values:\n",
        "                    for epoch in HP_EPOCH.domain.values:\n",
        "                        for loss in HP_LOSS.domain.values:         \n",
        "                                hparams = {\n",
        "                                    HP_INPUT: input1,\n",
        "                                    HP_HIDDEN1: hidden1,\n",
        "                                    HP_OPTIMIZER: optimizer,\n",
        "                                    HP_ACTIVATION: activation,\n",
        "                                    HP_EPOCH: epoch,\n",
        "                                    HP_LOSS: loss,\n",
        "                                    HP_HIDDEN2: hidden2,                                   \n",
        "                                }\n",
        "                                # Run the hyperparameters \n",
        "                                run_name = \"run-%d\" % session_num \n",
        "                                print('--- Starting trial: %s' % run_name) \n",
        "                                print({h.name: hparams[h] for h in hparams}) \n",
        "                                run('./hparam_tuning/' + run_name, hparams)\n",
        "                                session_num += 1 # Add session_num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rPnjDjxkLhM",
        "colab_type": "text"
      },
      "source": [
        "## 3 Hidden Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFRLdE_WkLAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # too technical right now\n",
        "# With statement\n",
        "with tf.summary.create_file_writer('./hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "      #hparams=[HP_INPUT,HP_HIDDEN1,HP_OPTIMIZER, HP_ACTIVATION, HP_EPOCH, HP_LOSS],\n",
        "    hparams=[HP_INPUT,HP_HIDDEN1, HP_HIDDEN2, HP_HIDDEN3,  HP_OPTIMIZER, HP_ACTIVATION, HP_EPOCH, HP_LOSS],\n",
        "    metrics=[hp.Metric('loss', display_name='loss'),\n",
        "             hp.Metric('recall', display_name='recall'),\n",
        "             hp.Metric('precision', display_name='precision'),\n",
        "             hp.Metric('f1', display_name='f1_score'),\n",
        "             hp.Metric(ACC_METRIC, display_name='accuracy')]\n",
        "  )\n",
        "\n",
        "# This function will execute the model builing method with different set of paratemer and log the results to the folder.\n",
        "# With statement helps you to keep your log file open while executing the commands in the with block\n",
        "def run(run_dir, hparams):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # Record the values used in this trial\n",
        "    loss, recall, precision, f1, accuracy = build_model(hparams)\n",
        "    tf.summary.scalar('loss', loss, step=1)\n",
        "    tf.summary.scalar('recall', recall, step=1) # Scalar: a data class; different value\n",
        "    tf.summary.scalar('precision', precision, step=1) # Scalar: a data class; different value\n",
        "    tf.summary.scalar('f1', f1, step=1)\n",
        "    tf.summary.scalar(ACC_METRIC, accuracy, step=1)\n",
        "\n",
        "# Run our model by using hyperparameters that defined earlier\n",
        "# Combined all of them in for loops to run the different combinations of parameters\n",
        "session_num = 0\n",
        "for input1 in HP_INPUT.domain.values:\n",
        "  for hidden1 in HP_HIDDEN1.domain.values:\n",
        "      for hidden2 in HP_HIDDEN2.domain.values:\n",
        "          for hidden3 in HP_HIDDEN3.domain.values:\n",
        "            for optimizer in HP_OPTIMIZER.domain.values:\n",
        "                for activation in HP_ACTIVATION.domain.values:\n",
        "                    for epoch in HP_EPOCH.domain.values:\n",
        "                        for loss in HP_LOSS.domain.values:\n",
        "          \n",
        "                                hparams = {\n",
        "                                    HP_INPUT: input1,\n",
        "                                    HP_HIDDEN1: hidden1,\n",
        "                                    HP_OPTIMIZER: optimizer,\n",
        "                                    HP_ACTIVATION: activation,\n",
        "                                    HP_EPOCH: epoch,\n",
        "                                    HP_LOSS: loss,\n",
        "                                    HP_HIDDEN2: hidden2,\n",
        "                                    HP_HIDDEN3: hidden3                                    \n",
        "                                }\n",
        "                                run_name = \"run-%d\" % session_num \n",
        "                                print('--- Starting trial: %s' % run_name) \n",
        "                                print({h.name: hparams[h] for h in hparams}) # this runs the parameters\n",
        "                                run('./hparam_tuning/' + run_name, hparams)\n",
        "                                session_num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO1nKLdK-kcM",
        "colab_type": "text"
      },
      "source": [
        "# **Testing & Experiemental Results**\n",
        "Use TensorBoard\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc5kYK2RSh3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loads an inplace TensorBoard to investigate the performance of  runs\n",
        "# This can sort performance\n",
        "# It allows you to compare the various hyperparameters\n",
        "%tensorboard --logdir ./hparam_tuning"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}